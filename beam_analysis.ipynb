{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.colors import LogNorm, LinearSegmentedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.signal\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import functools\n",
    "import itertools as it\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIX_SIZE = .00112\n",
    "\n",
    "DIR = os.path.join(os.getenv('HOME'), 'beam_sim')\n",
    "TAG = 'protons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variables from config file\n",
    "f_cfg = np.load('{}/{}_config.npz'.format(DIR, TAG))\n",
    "\n",
    "RES_X, RES_Y = f_cfg['res']\n",
    "FPS = f_cfg['fps']\n",
    "PARTICLES = f_cfg['particles']\n",
    "\n",
    "f_cfg.close()\n",
    "\n",
    "LIM_X = PIX_SIZE * RES_X / 2\n",
    "LIM_Y = PIX_SIZE * RES_Y / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Measuring Background #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = {}\n",
    "n_frames = {}\n",
    "\n",
    "NOISE = {}\n",
    "\n",
    "for fname in os.listdir(os.path.join(DIR, 'bg')):\n",
    "    p = fname.split('_')[0]\n",
    "    if not p in histograms:\n",
    "        histograms[p] = 0\n",
    "        n_frames[p] = 0\n",
    "    \n",
    "    f = np.load(os.path.join(DIR, 'bg', fname))\n",
    "    \n",
    "    histograms[p] += csr_matrix((np.ones(f['x'].size), (f['x'], f['y'])), shape=(RES_X,RES_Y))\n",
    "    n_frames[p] += 1\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "downsample = 97\n",
    "\n",
    "for p,h in histograms.items():\n",
    "    downscale = h.toarray().reshape(RES_X//downsample, downsample, RES_Y//downsample, downsample).sum((1,3))\n",
    "    downscale = scipy.signal.convolve2d(downscale, np.ones((3,3))/9, mode='same', boundary='symm')\n",
    "    NOISE[p] = cv2.resize(downscale, (RES_Y, RES_X)) / n_frames[p] / downsample**2\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(NOISE[p].transpose(), cmap='viridis')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Sorting the Data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to sort the phone data into spills by time.  But this first requires doing drift correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spill(dict):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def append(self, phone, t):\n",
    "        if not phone in self.keys():\n",
    "            self[phone] = [t]\n",
    "        else:\n",
    "            self[phone].append(t)\n",
    "            \n",
    "    def get_file(phone, t, filetype='cluster'):\n",
    "        return np.load(os.path.join(DIR, TAG, phone, filetype, '{}.npz'.format(t)))\n",
    "    \n",
    "    def histogram(self, phone, downsample=4):\n",
    "        hist = 0\n",
    "        \n",
    "        for t in self[phone]:\n",
    "            f = Spill.get_file(phone, t, filetype='cluster')\n",
    "            res_y_down = RES_Y // downsample\n",
    "            res_x_down = RES_X // downsample\n",
    "            hist += np.histogram2d(f.f.y, f.f.x, bins=(res_y_down, res_x_down), range=((0, RES_Y),(0, RES_X)))[0]\n",
    "            f.close()\n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phones_all = []\n",
    "t_nominal = []\n",
    "t_corr = []\n",
    "\n",
    "prefix_splits = PREFIX.count('_')\n",
    "\n",
    "for fname in glob(os.path.join(DIR, TAG, '*', 'cluster', '*')):\n",
    "    head, base = os.path.split(fname)\n",
    "    head, _ = os.path.split(head)\n",
    "    head, phone = os.path.split(head)\n",
    "    \n",
    "    t = int(base[:-4])\n",
    "    \n",
    "    # in case this is run multiple times, we use the original timestamp saved in the raw file\n",
    "    f = np.load(os.path.join(DIR, 'cluster', fname))\n",
    "    \n",
    "    phones_all.append(iphone)\n",
    "    t_nominal.append(f.f.t)\n",
    "    t_corr.append(t)\n",
    "    f.close()\n",
    "    \n",
    "tmin = min(t_nominal)\n",
    "tmax = max(t_nominal)\n",
    "\n",
    "plt.title('Image times')\n",
    "plt.hist(t_nominal, bins=100, range=(tmin, tmin+600000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(t_nominal)\n",
    "t_nominal_sorted = np.array(t_nominal)[sorting]\n",
    "t_corr_sorted = np.array(t_corr)[sorting]\n",
    "phones_sorted = np.array(phones_all)[sorting]\n",
    "\n",
    "# pick a phone to which we map the other phones' coordinates\n",
    "PHONE_LIST = sorted(list(set(phones_all))) # remove copies\n",
    "ROOT = PHONE_LIST[0]\n",
    "OTHERS = PHONE_LIST.copy()\n",
    "OTHERS.remove(ROOT)\n",
    "print(PHONE_LIST)\n",
    "print(ROOT)\n",
    "\n",
    "COMBINATIONS = [list(map(frozenset, it.combinations(PHONE_LIST, i))) for i in range(len(PHONE_LIST)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average number of hits when the beam is on\n",
    "\n",
    "p_arr = np.array(phones_sorted)\n",
    "t_nominal_arr = np.array(t_nominal_sorted)\n",
    "t_corr_arr = np.array(t_corr_sorted)\n",
    "\n",
    "median_flux = {}\n",
    "\n",
    "for iphone in PHONE_LIST:\n",
    "    n_hits = []\n",
    "    t_nominal_iphone = t_nominal_arr[p_arr == iphone]\n",
    "    t_corr_iphone = t_corr_arr[p_arr == iphone]\n",
    "    for t in t_corr_iphone:\n",
    "        f = Spill.get_file(iphone, t, 'cluster')\n",
    "        n_hits.append(len(f.f.x))\n",
    "    plt.hist(n_hits, bins=50, histtype='step', label=iphone[:6])\n",
    "    plt.xlabel('Hits per frame')\n",
    "    \n",
    "    # now exclude the first and last frame of each beam dump\n",
    "    t_diffs = np.hstack([[1e5], np.diff(t_nominal_iphone), [1e5]]) \n",
    "    \n",
    "    flux_full = np.array(n_hits)[(t_diffs[1:] < 1.5e3 / FPS) & (t_diffs[:-1] < 1.5e3 / FPS)]\n",
    "    \n",
    "    median_flux[iphone] = np.median(flux_full) \n",
    "    if noise:\n",
    "        median_flux[iphone] -= NOISE[iphone].sum()\n",
    "    \n",
    "plt.legend(loc='upper left');\n",
    "\n",
    "print(median_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the estimated start/end intervals for each spill, by phone\n",
    "\n",
    "spill_intervals = {}\n",
    "\n",
    "for iphone in PHONE_LIST:\n",
    "    t_corr_iphone = t_corr_arr[p_arr == iphone]\n",
    "    t_nominal_iphone = t_nominal_arr[p_arr == iphone]\n",
    "    t_diffs = np.hstack([[1e5], np.diff(t_nominal_iphone), [1e5]])\n",
    "    \n",
    "    t_len = t_corr_iphone.size\n",
    "    start_args = np.arange(t_len)[(t_diffs[1:] <= 5e3) & (t_diffs[:-1] > 5e3)]\n",
    "    end_args = np.arange(t_len)[(t_diffs[1:] > 5e3) & (t_diffs[:-1] <= 5e3)]\n",
    "    \n",
    "    t_start_corr = []\n",
    "    t_end_corr = []\n",
    "    \n",
    "    bg = NOISE[iphone].sum() if NOISE else 0\n",
    "    \n",
    "    for arg in start_args:\n",
    "        f = Spill.get_file(iphone, t_corr_iphone[arg], 'cluster')\n",
    "        t_start_corr.append(t_nominal_iphone[arg] + 1000 / FPS * (1 - (len(f.f.x) - bg) / median_flux[iphone]))\n",
    "        f.close()\n",
    "        \n",
    "    for arg in end_args: \n",
    "        f = Spill.get_file(iphone, t_corr_iphone[arg], 'cluster')\n",
    "        t_end_corr.append(t_nominal_iphone[arg] + 1000 / FPS * (len(f.f.x) - bg) / median_flux[iphone])\n",
    "        f.close()\n",
    "        \n",
    "    spill_intervals[iphone] = np.vstack([t_start_corr, t_end_corr])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we clean these up a bit\n",
    "    \n",
    "# first get rid of spills from extraneous triggering\n",
    "spill_lengths = np.array([])\n",
    "plt.xlabel('Calculated spill lengths (ms)');\n",
    "for iphone, intervals in spill_intervals.items():\n",
    "    l = intervals[1] - intervals[0]\n",
    "    plt.hist(l, bins=20, histtype='step', label=iphone[:6]);\n",
    "    plt.legend();\n",
    "    spill_lengths = np.append(l, spill_lengths)\n",
    "\n",
    "min_spill_length = np.mean(spill_lengths) / 2\n",
    "\n",
    "for iphone, intervals in spill_intervals.items():\n",
    "    l = intervals[1] - intervals[0]\n",
    "    spill_intervals[iphone] = spill_intervals[iphone][:, l > min_spill_length]\n",
    "    \n",
    "# then we get rid of extra spills at the beginning\n",
    "first_spills = [interval[0][0] for interval in spill_intervals.values()]\n",
    "min_spills = min([interval.shape[1] for interval in spill_intervals.values()])\n",
    "\n",
    "for iphone, intervals in spill_intervals.items():\n",
    "    spill_intervals[iphone] = spill_intervals[iphone][:, intervals[0] > max(first_spills) - 5000][:min_spills] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally get the linear coefficients\n",
    "times_root = spill_intervals[ROOT].flatten()\n",
    "times_root.sort()\n",
    "\n",
    "m = {ROOT: 1}\n",
    "b = {ROOT: 0}\n",
    "\n",
    "for other in OTHERS:\n",
    "    times_other = spill_intervals[other].flatten()\n",
    "    times_other.sort()\n",
    "    mi,bi,r,p,s = scipy.stats.linregress(times_other, times_root)\n",
    "    print(mi,bi)\n",
    "    m[other] = mi\n",
    "    b[other] = bi\n",
    "    \n",
    "    plt.xlabel(other[:6])\n",
    "    plt.ylabel(ROOT[:6])\n",
    "    plt.plot(times_other[-8:], times_other[-8:], 'r.', markersize=5, label='Nominal')\n",
    "    plt.plot(times_other[-8:], times_root[-8:], 'b.', markersize=5, label='Actual')\n",
    "    plt.plot(times_other[-8:], mi * times_other[-8:] + bi, lw=0.5, label='Correction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rename to reflect the adjusted times\n",
    "times = []\n",
    "phones = []\n",
    "\n",
    "for fname in os.listdir(DIR + '/cluster/'):\n",
    "    parts = (fname.split('.')[0]).split('_')\n",
    "    if len(parts) != 3: continue\n",
    "    iphone = parts[-2][1:]\n",
    "    t = int(parts[-1][1:])\n",
    "    f = np.load(os.path.join(DIR, 'cluster', fname))\n",
    "    t_corr = int(m[iphone] * f.f.t + b[iphone])\n",
    "    f.close()\n",
    "    \n",
    "    phones.append(iphone)\n",
    "    times.append(t_corr)\n",
    "    \n",
    "    f_full = os.path.join(DIR, 'cluster', fname)\n",
    "    os.rename(f_full, f_full.replace(str(t), str(t_corr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo sorting\n",
    "sorting = np.argsort(times)\n",
    "t_sorted = np.array(times)[sorting]\n",
    "phones_sorted = np.array(phones)[sorting]\n",
    "\n",
    "dt = np.diff(t_sorted)\n",
    "\n",
    "plt.title(r'$\\Delta t$ between images')\n",
    "plt.hist(dt, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we separate the various spills in the files\n",
    "\n",
    "def get_tuple(ientry):\n",
    "    return phones_sorted[ientry], t_sorted[ientry]\n",
    "\n",
    "n_phones = len(set(phones_sorted))\n",
    "spills = []\n",
    "spill_lengths = []\n",
    "imin = 0\n",
    "\n",
    "ispill = Spill()\n",
    "iphone, t = get_tuple(0)\n",
    "ispill.append(iphone, t)\n",
    "\n",
    "for i, delta in enumerate(dt):\n",
    "    iphone, t = get_tuple(i+1)\n",
    "    if delta > 5000:\n",
    "        spill_lengths.append(t_sorted[i] - t_sorted[imin])\n",
    "        imin = i+1\n",
    "        spills.append(ispill)\n",
    "        # create a new spill\n",
    "        ispill = Spill()\n",
    "    \n",
    "    ispill.append(iphone, t)\n",
    "\n",
    "spill_lengths.append(max(t_sorted) - t_sorted[imin])\n",
    "spills.append(ispill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Alignment #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truth values ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need coordinates that will be useful for finding the true mapping from sensor to lab.  In Eulerian angles, we have:\n",
    "\n",
    "$x_l=(x_s+x_0)(\\cos\\psi\\cos\\theta\\cos\\phi - \\sin\\psi\\sin\\phi) - (y_s+y_0)(\\sin\\psi\\cos\\theta\\cos\\phi -\\cos\\psi\\sin\\phi)$\n",
    "$y_l=(x_s+x_0)(\\cos\\psi\\cos\\theta\\sin\\phi + \\sin\\psi\\cos\\phi) - (y_s+y_0)(\\sin\\psi\\cos\\theta\\sin\\phi +\\cos\\psi\\cos\\phi)$\n",
    "\n",
    "First, we can redefine the offsets in terms of the lab frame:\n",
    "\n",
    "$x_l=x_{l0} + x_s(\\cos\\psi\\cos\\theta\\cos\\phi - \\sin\\psi\\sin\\phi) - y_s(\\sin\\psi\\cos\\theta\\cos\\phi -\\cos\\psi\\sin\\phi)$\n",
    "$y_l=y_{l0} + x_s(\\cos\\psi\\cos\\theta\\sin\\phi + \\sin\\psi\\cos\\phi) - y_s(\\sin\\psi\\cos\\theta\\sin\\phi +\\cos\\psi\\cos\\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the phones are more or less aligned, we expect that $\\psi\\approx -\\phi$.  If we write $\\psi=-\\phi+\\delta$, then in matrix notation:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "x_{l0} \\\\\n",
    "y_{l0}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi & -\\sin\\phi \\\\\n",
    "\\sin\\phi & \\cos\\phi\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\theta & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi & \\sin\\phi \\\\\n",
    "-\\sin\\phi & \\cos\\phi\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\delta & -\\sin\\delta \\\\\n",
    "\\sin\\delta & \\cos\\delta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_s \\\\\n",
    "y_s\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "The rightmost of these four transformations is a rotation in the lab space, whereas the others compress the sensor coordinates about some axis.  Multiplying these matrices together and simplifying, we see that:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi & -\\sin\\phi \\\\\n",
    "\\sin\\phi & \\cos\\phi\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\theta & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi & \\sin\\phi \\\\\n",
    "-\\sin\\phi & \\cos\\phi\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "- (1 - \\cos\\theta)\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi & \\sin\\phi\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos\\phi \\\\ \\sin\\phi\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "$= \\mathbf{1} - \\mathbf{v}\\cdot\\mathbf{v}^T$\n",
    "\n",
    "for some vector $\\mathbf{v}$ with components between 0 and 1.  While we do not know the offset, we can expect that $\\delta\\approx 0$ and $\\mathbf{v} \\approx \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore,\n",
    "\n",
    "$ (\\mathbf{1} - \\mathbf{v}\\cdot\\mathbf{v}^T)^{-1} = \\mathbf{1} + \\frac{\\mathbf{v}\\cdot\\mathbf{v}^T}{\\det(\\mathbf{1} - \\mathbf{v}\\cdot\\mathbf{v}^T)}$\n",
    "\n",
    "and so we can model the combined effect of two of these transformations as:\n",
    "\n",
    "$ UV^{-1} = \\mathbf{1} - \\mathbf{u}\\cdot\\mathbf{u}^T + \\mathbf{v'}\\cdot\\mathbf{v'}^T + O(\\mathbf{u}^2, \\mathbf{v'}^2) \\\\\n",
    "= \\mathbf{1} - \\begin{pmatrix}\n",
    "u_x & u_{xy} \\\\\n",
    "u_{xy} & u_y\n",
    "\\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the mapping parameters ###\n",
    "\n",
    "We can find a coarse estimate of $dx$ and $dy$ by comparing beam profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = {p:0 for p in PHONE_LIST}\n",
    "downsample=97\n",
    "noise_grids = {p: NOISE[p].reshape(RES_X//downsample, downsample, RES_Y//downsample, downsample).sum((1,3)).transpose() \\\n",
    "               for p in PHONE_LIST}\n",
    "\n",
    "for spl in spills:\n",
    "    for p in PHONE_LIST:\n",
    "        hist_i = spl.histogram(p, downsample=downsample)\n",
    "        hists[p] += hist_i - noise_grids[p]\n",
    "        \n",
    "for p in hists:\n",
    "    plt.figure()\n",
    "    plt.title(r'Flux in particles / (pixel $\\cdot$ s): {}'.format(p[:6]))\n",
    "    plt.imshow(hists[p] / downsample**2 / len(spills) / 4.2, cmap='plasma', origin='lower', extent=[0, RES_X, 0, RES_Y]);\n",
    "    plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisq_offset(hist1, hist2, dx, dy):\n",
    "    if dy > 0:\n",
    "        hist1 = hist1[dy:,:]\n",
    "        hist2 = hist2[:-dy, :]\n",
    "    elif dy < 0:\n",
    "        hist1 = hist1[:dy,:]\n",
    "        hist2 = hist2[-dy:, :]\n",
    "    \n",
    "    if dx > 0:\n",
    "        hist1 = hist1[:,dx:]\n",
    "        hist2 = hist2[:,:-dx]\n",
    "    elif dx < 0:\n",
    "        hist1 = hist1[:, :dx]\n",
    "        hist2 = hist2[:, -dx:]\n",
    "        \n",
    "    return np.sum((hist1/hist1.mean() - hist2/hist2.mean())**2/(hist1 + hist2)) / hist1.size\n",
    "\n",
    "min_intersection = 5\n",
    "shape = list(hists.values())[0].shape\n",
    "chisq_grids = {c: np.array([[chisq_offset(hists[sorted(c)[0]], hists[sorted(c)[1]], x, y) \\\n",
    "                       for x in np.arange(-shape[1]+min_intersection, shape[1]-min_intersection+1)] \\\n",
    "                       for y in np.arange(-shape[0]+min_intersection, shape[0]-min_intersection+1)]) \\\n",
    "               for c in map(tuple, map(sorted, COMBINATIONS[2]))}\n",
    "                       \n",
    "sx = downsample*(shape[1] - 2*min_intersection)*PIX_SIZE \n",
    "sy = downsample*(shape[0] - 2*min_intersection)*PIX_SIZE\n",
    "\n",
    "dxy_all = {}\n",
    "\n",
    "for c, grid in chisq_grids.items():\n",
    "    plt.figure()\n",
    "    plt.title(r'$\\chi^2$: ({}, {})'.format(c[0][:6], c[1][:6]))\n",
    "    plt.xlabel('dx (mm)')\n",
    "    plt.ylabel('dy (mm)')\n",
    "    plt.imshow(grid, cmap='Spectral', norm=LogNorm(), extent=[-sx, sx, -sy, sy], origin='lower');\n",
    "    plt.colorbar();\n",
    "\n",
    "    iy, ix = np.unravel_index(np.argmin(grid), grid.shape)\n",
    "\n",
    "    dxy_all[c] = (ix / (grid.shape[1]-1) * 2 * sx - sx, iy / (grid.shape[0]-1) * 2 * sy - sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pick a root frame to map coordinates\n",
    "furthest = {}\n",
    "\n",
    "for c,xy in dxy_all.items():\n",
    "    dsq = xy[0]**2 + xy[1]**2\n",
    "    for p in c:\n",
    "        if not p in furthest or dsq > furthest[p]:\n",
    "            furthest[p] = dsq \n",
    "    \n",
    "ROOT = max(furthest.keys(), key=(lambda key: furthest[key]))\n",
    "OTHERS = [p for p in PHONE_LIST if p is not ROOT]\n",
    "    \n",
    "print(furthest)\n",
    "print('ROOT:', ROOT)\n",
    "print('OTHERS:', OTHERS)\n",
    "\n",
    "dxy_coarse = {}\n",
    "for c,xy in dxy_all.items():\n",
    "    if ROOT == c[0]:\n",
    "        dxy_coarse[c[1]] = xy\n",
    "    elif ROOT == c[1]:\n",
    "        dxy_coarse[c[0]] = (-xy[0], -xy[1])\n",
    "        \n",
    "print(dxy_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we convert the truth results to these new coordinates with the given ROOT\n",
    "ftruth = np.load('{}/{}_truth.npz'.format(DIR, PREFIX))\n",
    "print(ftruth.f.hwid, ROOT)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# first draw beam profile\n",
    "spot_size = 6\n",
    "beam_xy = scipy.stats.multivariate_normal(mean=[0,0], cov=spot_size/2)\n",
    "n_particles=5e5\n",
    "sz = 1.5*spot_size\n",
    "x = np.linspace(-sz/2, sz/2, 100)\n",
    "y = np.linspace(-sz/2, sz/2, 100)\n",
    "pos = np.dstack(np.meshgrid(x,y))\n",
    "intensity = n_particles*beam_xy.pdf(pos)\n",
    "plt.imshow(intensity, extent=2*[-sz/2, sz/2], cmap='plasma', origin='lower')\n",
    "plt.colorbar()\n",
    "\n",
    "# next add rectangles for each phone\n",
    "n_phones = ftruth.f.hwid.size\n",
    "\n",
    "rects = np.repeat(np.array([[\n",
    "        [-LIM_X, -LIM_Y],\n",
    "        [LIM_X, -LIM_Y],\n",
    "        [LIM_X, LIM_Y],\n",
    "        [-LIM_X, LIM_Y]]]), n_phones, axis=0)\n",
    "\n",
    "# do rotations\n",
    "\n",
    "cos_phi = np.cos(ftruth.f.phi)\n",
    "sin_phi = np.sin(ftruth.f.phi)\n",
    "phi_mat = np.array([[cos_phi, -sin_phi], [sin_phi, cos_phi]])\n",
    "\n",
    "theta_mat = np.array([\n",
    "    [np.cos(ftruth.f.theta), np.zeros(n_phones)],\n",
    "    [np.zeros(n_phones), np.ones(n_phones)]\n",
    "    ])\n",
    "\n",
    "cos_psi = np.cos(ftruth.f.psi)\n",
    "sin_psi = np.sin(ftruth.f.psi)\n",
    "psi_mat = np.array([[cos_psi, -sin_psi], [sin_psi, cos_psi]])\n",
    "\n",
    "# need to rotate to frame of root phone for the right \"truth\" coords\n",
    "\n",
    "root_idx = np.argwhere(ftruth.f.hwid == ROOT)[0]\n",
    "delta0 = ftruth.f.phi[root_idx] + ftruth.f.psi[root_idx]\n",
    "cos_delta = np.cos(delta0)[0]\n",
    "sin_delta = np.sin(delta0)[0]\n",
    "delta_mat = np.array([[cos_delta, sin_delta], [-sin_delta, cos_delta]])\n",
    "\n",
    "xy_lab_truth = dict(zip(ftruth.f.hwid, np.dstack([ftruth.f.x, ftruth.f.y])[0]))\n",
    "\n",
    "for iphone, phone in enumerate(ftruth.f.hwid):\n",
    "\n",
    "    for corner in range(4):\n",
    "        psi_rot_c = np.matmul(psi_mat[:,:,iphone], rects[iphone][corner])\n",
    "        theta_rot_c = np.matmul(theta_mat[:,:,iphone], psi_rot_c)\n",
    "        rects[iphone][corner] = np.matmul(phi_mat[:,:,iphone], theta_rot_c) + xy_lab_truth[phone]\n",
    "\n",
    "\n",
    "phi_adj = np.mod(ftruth.f.phi, np.pi) - np.pi # same transformation mod pi\n",
    "\n",
    "v_truth = dict(zip(ftruth.f.hwid, \\\n",
    "        np.sqrt(1-np.cos(ftruth.f.theta)).reshape(-1,1) * np.dstack([np.cos(phi_adj), np.sin(phi_adj)])[0]))\n",
    "vinv_ROOT = v_truth[ROOT]/np.sqrt(1 - np.inner(v_truth[ROOT], v_truth[ROOT]))\n",
    "scale_root = np.eye(2) + np.outer(vinv_ROOT, vinv_ROOT)\n",
    "scale_mats = {p: np.eye(2) - np.outer(v_truth[p], v_truth[p]) for p in PHONE_LIST}\n",
    "\n",
    "print(\"XY:\")\n",
    "print(\"abs\")\n",
    "print(np.array(list(xy_lab_truth.values())))\n",
    "dxy_truth = {p: functools.reduce(np.matmul, [scale_root, delta_mat, xy_lab_truth[p] - xy_lab_truth[ROOT]]) for p in PHONE_LIST}\n",
    "print(\"diff\")\n",
    "print(np.array(list(dxy_truth.values())))\n",
    "print()\n",
    "\n",
    "print(\"phi:\")\n",
    "print(ftruth.f.phi)\n",
    "phi_lab_truth = dict(zip(ftruth.f.hwid, ftruth.f.phi + ftruth.f.psi))\n",
    "print(\"abs\")\n",
    "print(np.array(list(phi_lab_truth.values())))\n",
    "dphi_truth = {p: phi_lab_truth[p] - phi_lab_truth[ROOT] for p in PHONE_LIST}\n",
    "print(\"diff\")\n",
    "print(np.array(list(dphi_truth.values())))\n",
    "print()\n",
    "\n",
    "print(\"u:\")\n",
    "u_truth = {p: np.array([v_truth[p][0]**2 - vinv_ROOT[0]**2, \\\n",
    "                        v_truth[p][0] * v_truth[p][1] - vinv_ROOT[0] * vinv_ROOT[1], \\\n",
    "                        v_truth[p][1]**2 - vinv_ROOT[1]**2]) \\\n",
    "           for p in PHONE_LIST}\n",
    "#u_truth = {p: np.dot(scale_root, scale_mats[p]) for p in PHONE_LIST}\n",
    "print(np.array(list(u_truth.values())))\n",
    "\n",
    "align_truth = {p: np.hstack([dxy_truth[p], dphi_truth[p], u_truth[p]]) for p in PHONE_LIST}\n",
    "\n",
    "polygons = [Polygon(rects[iphone]) for iphone in range(n_phones)]\n",
    "p = PatchCollection(polygons, edgecolors='r', facecolors='none', linewidths=1)\n",
    "ax.add_collection(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can do some finer adjustment which includes $\\phi$.  Calculating the cdf for the nearest pixel from another frame:\n",
    "\n",
    "$dp = 2\\pi n r dr$\n",
    "\n",
    "$1 - F(r) = \\prod_{0}^{r}(1-dp) = \\exp(-\\int_{0}^{r}2\\pi n r` dr`) = \\exp(-\\pi n r^2)$\n",
    "\n",
    "$f(r) = 2\\pi n r\\exp(-\\pi n r^2)$\n",
    "\n",
    "$\\langle r \\rangle = \\int_{0}^{\\infty} 2 \\pi n r^2 \\exp(-\\pi n r^2) dr$\n",
    "\n",
    "$ = \\frac{1}{\\sqrt{\\pi n}}\\int_{-\\infty}^{\\infty}s^2 \\exp(-s^2) ds$\n",
    "\n",
    "$ = \\frac{1}{2\\sqrt{n}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for converting between frames\n",
    "\n",
    "def sensor_map(xs, ys, dx=0, dy=0, phi=0, ux=0, uxy=0, uy=0):\n",
    "    xy_s = np.array([xs, ys])\n",
    "    phi_mat = np.array([\n",
    "            [np.cos(phi), -np.sin(phi)], \n",
    "            [np.sin(phi), np.cos(phi)]\n",
    "        ])\n",
    "    \n",
    "    scale = np.eye(2) - np.array([[ux, uxy],[uxy, uy]])\n",
    "    \n",
    "    return functools.reduce(np.dot, [scale, phi_mat, xy_s]) + np.array([[dx], [dy]])\n",
    "\n",
    "def inverse_map(xlab, ylab, dx=0, dy=0, phi=0, ux=0, uxy=0, uy=0):\n",
    "    xy_lab = np.array([xlab, ylab]) - np.array([[dx], [dy]])\n",
    "    inv_phi_mat = np.array([\n",
    "            [np.cos(phi), np.sin(phi)],\n",
    "            [-np.sin(phi), np.cos(phi)]\n",
    "        ])\n",
    "    \n",
    "    inv_scale = np.linalg.inv(np.eye(2) - np.array([[ux, uxy],[uxy, uy]]))\n",
    "    \n",
    "    return functools.reduce(np.dot, [inv_phi_mat, inv_scale, xy_lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for sorting points for easier computation\n",
    "def divide_points(x, y, ndivs, limits=None):\n",
    "    if limits:\n",
    "        xmin, xmax, ymin, ymax = limits\n",
    "    else:\n",
    "        xmin = np.amin(x)\n",
    "        xmax = np.amax(x)\n",
    "        ymin = np.amin(y)\n",
    "        ymax = np.amin(y)\n",
    "        \n",
    "    divx = (xmax - xmin) / ndivs\n",
    "    divy = (ymax - ymin) / ndivs\n",
    "    \n",
    "    return np.minimum((x - xmin) // divx, ndivs-1), np.minimum((y - ymin) // divy, ndivs-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we introduce a suitable scoring function for alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_points(x1, y1, x2, y2, alpha=0.5, ndivs=13, hist_bins=100):\n",
    "        \n",
    "    xmin = min(np.amin(x1), np.amin(x2))\n",
    "    xmax = max(np.amax(x1), np.amax(x2))\n",
    "    ymin = min(np.amin(y1), np.amin(y2))\n",
    "    ymax = max(np.amax(y1), np.amax(y2))\n",
    "    \n",
    "    divx = (xmax - xmin) / ndivs\n",
    "    divy = (ymax - ymin) / ndivs\n",
    "    \n",
    "    # make interlaced cells\n",
    "    x1_cells, y1_cells = divide_points(x1, y1, ndivs, limits=(xmin, xmax, ymin, ymax))\n",
    "    x2_cells, y2_cells = divide_points(x2, y2, ndivs+1, limits=(xmin - divx/2, xmax + divx/2, ymin-divy/2, ymax+divy/2))\n",
    "    \n",
    "    total_score = 0\n",
    "    survival_hist = np.zeros(hist_bins)\n",
    "    \n",
    "    for i,j in np.ndindex(ndivs, ndivs):\n",
    "        group1 = (x1_cells == i) & (y1_cells == j) \n",
    "        if not group1.sum():\n",
    "            continue\n",
    "        \n",
    "        group2 = ((x2_cells == i) | (x2_cells == i+1)) & ((y2_cells == j) | (y2_cells == j+1))\n",
    "        if not group2.sum():\n",
    "            continue\n",
    "        \n",
    "        rsquared_min = np.amin((x1[group1] - x2[group2].reshape(-1,1))**2 \\\n",
    "                               + (y1[group1] - y2[group2].reshape(-1,1))**2, axis=0)\n",
    "    \n",
    "        # this is just cdf for min Euclidean distance\n",
    "        density = group2.sum() / (4 * divx * divy)\n",
    "        survival = np.exp(-rsquared_min * np.pi * density)\n",
    "        scores = np.maximum(survival - (1 - alpha), 0) / alpha\n",
    "        total_score += np.sum(scores)\n",
    "        survival_hist += np.histogram(survival, bins=hist_bins, range=(0,1))[0]\n",
    "           \n",
    "    return total_score / len(x1), survival_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(fps, *t):\n",
    "    diff = max(t) - min(t)\n",
    "    return max(1 - fps * diff / 1000, 0)\n",
    "        \n",
    "\n",
    "def score_spills(p1, p2, *spills, dx=0, dy=0, phi=0, ux=0, uxy=0, uy=0, nmax=None, **kwargs):\n",
    "    scores = 0\n",
    "    total = 0\n",
    "    n = 0\n",
    "    \n",
    "    for spl in spills:\n",
    "        # to save time in coarse adjustment\n",
    "        if nmax and n >= nmax: break\n",
    "            \n",
    "        spl1 = spl[p1].copy()\n",
    "        spl2 = spl[p2].copy()\n",
    "        \n",
    "        t1 = spl1.pop(0)\n",
    "        t2 = spl2.pop(0)\n",
    "        \n",
    "        while spl1 and spl2:\n",
    "            if nmax and n >= nmax: break\n",
    "                \n",
    "            overlap = calculate_overlap(FPS, t1, t2)\n",
    "            if overlap:\n",
    "                \n",
    "                n += 1\n",
    "\n",
    "                f1 = Spill.get_file(p1, t1, 'cluster')\n",
    "                f2 = Spill.get_file(p2, t2, 'cluster')\n",
    "                       \n",
    "                x1_sensor1 = (f1['x'] + 0.5) * PIX_SIZE - LIM_X\n",
    "                y1_sensor1 = (f1['y'] + 0.5) * PIX_SIZE - LIM_Y\n",
    "                x2_sensor2 = (f2['x'] + 0.5) * PIX_SIZE - LIM_X\n",
    "                y2_sensor2 = (f2['y'] + 0.5) * PIX_SIZE - LIM_Y\n",
    "\n",
    "                x2_sensor1, y2_sensor1 = sensor_map(x2_sensor2, y2_sensor2, dx=dx, dy=dy, phi=phi, ux=ux, uxy=uxy, uy=uy)\n",
    "                \n",
    "                # limit to hits that map to opposite sensor as well\n",
    "                x1_sensor2, y1_sensor2 = inverse_map(x1_sensor1, y1_sensor1, dx=dx, dy=dy, phi=phi, ux=ux, uxy=uxy, uy=uy)\n",
    "\n",
    "                intersect1 = (np.abs(x1_sensor2) < LIM_X) & (np.abs(y1_sensor2) < LIM_Y)\n",
    "                intersect2 = (np.abs(x2_sensor1) < LIM_X) & (np.abs(y2_sensor1) < LIM_Y)\n",
    "\n",
    "                x1_sensor1 = x1_sensor1[intersect1]\n",
    "                y1_sensor1 = y1_sensor1[intersect1]\n",
    "                x2_sensor1 = x2_sensor1[intersect2]\n",
    "                y2_sensor1 = y2_sensor1[intersect2]\n",
    "\n",
    "                score, _ = score_points(x1_sensor1, y1_sensor1, x2_sensor1, y2_sensor1, **kwargs)\n",
    "                \n",
    "                scores += overlap*score\n",
    "                total += overlap\n",
    "            \n",
    "            if t1 > t2:\n",
    "                t2 = spl2.pop(0)\n",
    "            else:\n",
    "                t1 = spl1.pop(0)\n",
    "\n",
    "    return scores / total\n",
    "\n",
    "print(score_spills(ROOT, OTHERS[1], *spills, alpha=0.3, ndivs=13, nmax=5))\n",
    "%timeit score_spills(ROOT, OTHERS[1], *spills, alpha=0.3, ndivs=13, nmax=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDIVS = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our scoring function on the provided truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_scores = {other: score_spills(ROOT, other, *spills[:5], dx=dxy_truth[other][0], dy=dxy_truth[other][1], phi=dphi_truth[other], \\\n",
    "             ux=u_truth[other][0], uxy=u_truth[other][1], uy=u_truth[other][2], alpha=.3) for other in OTHERS}\n",
    "print(max_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_grid(p1, p2, spills, dx, dy, delta, res, alpha=0.5, visualize=False, **kwargs):\n",
    "    x_bins, y_bins = np.meshgrid(np.linspace(dx-delta, dx+delta, res), np.linspace(dy-delta, dy+delta, res))\n",
    "    score_grid = np.array([[score_spills(p1, p2, *spills, dx=x, dy=y, alpha=alpha, ndivs=NDIVS, **kwargs) \\\n",
    "                   for x in np.linspace(dx-delta, dx+delta, res)] \\\n",
    "                  for y in np.linspace(dy-delta, dy+delta, res)])\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure()\n",
    "        plt.imshow(score_grid, extent=[dx-delta, dx+delta, dy-delta, dy+delta], cmap='seismic', \\\n",
    "                   origin='lower');\n",
    "        plt.title('Truth ({}, {})'.format(p1[:6], p2[:6]))\n",
    "        plt.xlabel('dx (mm)')\n",
    "        plt.ylabel('dy (mm)')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return score_grid, x_bins, y_bins\n",
    "\n",
    "# test with truth values\n",
    "for other in OTHERS:\n",
    "    xy_grid(ROOT, other, spills, *(dxy_truth[other]), .005, 13, phi=dphi_truth[other], \\\n",
    "            ux=u_truth[other][0], uxy=u_truth[other][1], uy=u_truth[other][2], \\\n",
    "            alpha=0.1, nmax=3, visualize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_grid(p1, p2, spills, phi, delta, res, visualize=True, **kwargs):\n",
    "    plt.figure()\n",
    "    phi_bins = np.linspace(phi-delta, phi+delta, res)\n",
    "    score_grid = np.array([score_spills(p1, p2, *spills, phi=phi_i, ndivs=NDIVS, **kwargs) for phi_i in phi_bins])\n",
    "    \n",
    "    if visualize:\n",
    "        plt.plot(phi_bins, score_grid)\n",
    "        plt.title('Truth ({}, {})'.format(p1[:6], p2[:6]))\n",
    "        plt.xlabel(r'$\\Delta\\phi$')\n",
    "        plt.ylabel('Score')\n",
    "        plt.show()\n",
    "    \n",
    "    return score_grid, phi_bins\n",
    "    \n",
    "for other in OTHERS:\n",
    "    phi_grid(ROOT, other, spills, dphi_truth[other], .01, 25, dx=dxy_truth[other][0], dy=dxy_truth[other][1], \\\n",
    "            ux=u_truth[other][0], uxy=u_truth[other][1], uy=u_truth[other][2], nmax=3, alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(grid, *bins):\n",
    "    idx = np.unravel_index(np.argmax(grid), grid.shape)\n",
    "    return tuple(b[idx] for b in bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a combination of grid searches and Nelder-Mead optimization, we can find approximate $(x, y, \\phi)$ coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with increasingly precise grid searches in x,y,phi\n",
    "xyphi = [{}]\n",
    "\n",
    "def xy_phi_grid(p1, p2, spills, dx_guess, dy_guess, phi_guess, delta_xy, delta_phi, res_xy=11, res_phi=15, \\\n",
    "                visualize=None, **kwargs):\n",
    "\n",
    "    if visualize and visualize != 'best' and visualize != 'all': \n",
    "        raise ValueError('\"visualize\" must be one of \"best\"/\"all\"')\n",
    "    \n",
    "    dx_result = dx_guess\n",
    "    dy_result = dy_guess\n",
    "    phi_result = phi_guess\n",
    "    score_grid_best = None\n",
    "\n",
    "    max_score = 0\n",
    "    for phi_i in np.linspace(phi_guess-delta_phi, phi_guess+delta_phi, res_phi):\n",
    "        if visualize=='all':\n",
    "            print(\"phi={:.4f}\".format(phi_i))\n",
    "        score_grid, x_bins, y_bins = xy_grid(p1, p2, spills, dx_guess, dy_guess, delta_xy, res_xy, \\\n",
    "                                    phi=phi_i, visualize=(visualize=='all'), **kwargs)\n",
    "\n",
    "        grid_max = np.amax(score_grid)\n",
    "        if grid_max > max_score:\n",
    "            max_score = grid_max\n",
    "\n",
    "            dx_result, dy_result = get_max(score_grid, x_bins, y_bins)\n",
    "\n",
    "            phi_result = phi_i\n",
    "            \n",
    "            score_grid_best = score_grid\n",
    "\n",
    "    if visualize == 'best':\n",
    "        plt.figure()\n",
    "        plt.xlabel('dx (mm)')\n",
    "        plt.ylabel('dy (mm)')\n",
    "        plt.imshow(score_grid_best, extent=[dx_guess-delta_xy, dx_guess+delta_xy, dy_guess-delta_xy, dy_guess+delta_xy], \\\n",
    "                   cmap='seismic', origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.title(r'$\\phi = {:.4}, ({}, {})$'.format(phi_result, p1[:6], p2[:6]))\n",
    "        plt.show()\n",
    "            \n",
    "    return dx_result, dy_result, phi_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 0\n",
    "\n",
    "# do the initial iteration separately\n",
    "xyphi[ITER][OTHERS[0]] = xy_phi_grid(ROOT, OTHERS[0], spills, *dxy_coarse[OTHERS[0]], 0, 0.5, 0.15, 31, 15, alpha=0.3, nmax=3, visualize='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 0\n",
    "xyphi[ITER][OTHERS[1]] = xy_phi_grid(ROOT, OTHERS[1], spills, *dxy_coarse[OTHERS[1]], 0, 0.5, 0.15, 31, 15, alpha=0.3, nmax=3, visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 0\n",
    "xyphi[ITER][OTHERS[2]] = xy_phi_grid(ROOT, OTHERS[2], spills, *dxy_coarse[OTHERS[2]], 0, 0.5, 0.15, 31, 15, alpha=0.3, nmax=3, visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 1\n",
    "\n",
    "if len(xyphi) <= ITER:\n",
    "    xyphi.append({})\n",
    "else:\n",
    "    xyphi[ITER] = {}\n",
    "\n",
    "for other in OTHERS:\n",
    "    xyphi[ITER][other] = xy_phi_grid(ROOT, other, spills, *xyphi[ITER-1][other], 0.06, .02, 15, 9, alpha=0.1, nmax=5, visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 2\n",
    "\n",
    "if len(xyphi) <= ITER:\n",
    "    xyphi.append({})\n",
    "else:\n",
    "    xyphi[ITER] = {}\n",
    "\n",
    "for other in OTHERS:\n",
    "    xyphi[ITER][other] = xy_phi_grid(ROOT, other, spills, *xyphi[ITER-1][other], .02, .005, 15, 9, alpha=0.1, nmax=5, visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have a good enough initial guess, we can use an optimizer to find the maximum\n",
    "\n",
    "ITER = 3\n",
    "\n",
    "def f_xyphi(x, p1, p2, spills):\n",
    "    return 1 - score_spills(p1, p2, *spills, dx=x[0], dy=x[1], phi=x[2], nmax=25, alpha=0.1)\n",
    "\n",
    "if len(xyphi) <= ITER:\n",
    "    xyphi.append({})\n",
    "else:\n",
    "    xyphi[ITER] = {}\n",
    "\n",
    "for other in OTHERS:\n",
    "    guess = np.array(xyphi[ITER-1][other])\n",
    "    progress = [guess]\n",
    "    res = scipy.optimize.minimize(f_xyphi, guess, args=(ROOT, other, spills), method='Nelder-Mead', callback = lambda xk: progress.append(xk))\n",
    "    \n",
    "    progress_arr = np.array(progress).transpose()\n",
    "    \n",
    "    xmin = np.amin(progress_arr[0])\n",
    "    xmax = np.amax(progress_arr[0])\n",
    "    ymin = np.amin(progress_arr[1])\n",
    "    ymax = np.amax(progress_arr[1])\n",
    "    \n",
    "    plt.xlim(xmin-(xmax-xmin)/6, xmax+(xmax-xmin)/6)\n",
    "    plt.ylim(ymin-(ymax-ymin)/6, ymax+(ymax-ymin)/6)\n",
    "    \n",
    "    plt.title(r'$(x, y, \\phi)$ Convergence: ({}, {})'.format(ROOT[:6], other[:6]))\n",
    "    plt.xlabel('dx (mm)')\n",
    "    plt.ylabel('dy (mm)')\n",
    "    plt.scatter(progress_arr[0], progress_arr[1], c=progress_arr[2], \\\n",
    "                s=160*np.arange(progress_arr.shape[1]+1)[::-1] / progress_arr.shape[1], cmap='winter', marker='x')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    if res.success:\n",
    "        print(\"Success!\")\n",
    "        dx, dy, phi = res.x\n",
    "        xyphi[-1][other] = (dx, dy, phi)\n",
    "    else: print(\"Failed:\", res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a similar process for $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def u_grid(p1, p2, spills, ux, uxy, uy, delta, res, visualize=None, **kwargs):\n",
    "\n",
    "    if visualize and visualize != 'best' and visualize != 'all': \n",
    "        raise ValueError('\"visualize\" must be one of \"best\"/\"all\"')\n",
    "    \n",
    "    ux_result = ux\n",
    "    uxy_result = uxy\n",
    "    uy_result = uy\n",
    "    score_grid_best = None\n",
    "\n",
    "    max_score = 0\n",
    "    for uxy_i in np.linspace(uxy-delta, uxy+delta, res):\n",
    "        \n",
    "        score_grid = np.array([[score_spills(p1, p2, *spills, ux=ux_i, uxy=uxy_i, uy=uy_i, **kwargs) \\\n",
    "                   for ux_i in np.linspace(ux-delta, ux+delta, res)] \\\n",
    "                  for uy_i in np.linspace(uy-delta, uy+delta, res)])\n",
    "        \n",
    "        if visualize=='all':\n",
    "            plt.figure()\n",
    "            plt.imshow(score_grid, extent=[ux-delta, ux+delta, uy-delta, uy+delta], \\\n",
    "                       cmap='seismic', origin='lower')\n",
    "            plt.colorbar()\n",
    "            plt.title(r'$u_{xy} =$' + '{:.4}'.format(uxy_i))\n",
    "            plt.show()\n",
    "\n",
    "        grid_max = np.amax(score_grid)\n",
    "        if grid_max > max_score:\n",
    "            max_score = grid_max\n",
    "\n",
    "            ux_bins, uy_bins = np.meshgrid(np.linspace(ux-delta, ux+delta, res), np.linspace(uy-delta, uy+delta, res))\n",
    "            ux_result, uy_result = get_max(score_grid, ux_bins, uy_bins)\n",
    "\n",
    "            uxy_result = uxy_i\n",
    "            \n",
    "            score_grid_best = score_grid\n",
    "\n",
    "    if visualize == 'best':\n",
    "        plt.figure()\n",
    "        plt.imshow(score_grid_best, extent=[ux-delta, ux+delta, uy-delta, uy+delta], \\\n",
    "                   cmap='seismic', origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.title(r'$u_{xy} =$' + '{:.4}'.format(uxy_result))\n",
    "        plt.xlabel(r'$u_x$')\n",
    "        plt.ylabel(r'$u_y$')\n",
    "        plt.show()\n",
    "            \n",
    "    return ux_result, uxy_result, uy_result\n",
    "\n",
    "u_guess = [{}]\n",
    "\n",
    "for other in OTHERS:\n",
    "    u_guess[0][other] = u_grid(ROOT, other, spills, 0, 0, 0, 1e-2, 11, nmax=5, visualize='best', alpha=0.05, \\\n",
    "           dx=xyphi[-1][other][0], dy=xyphi[-1][other][1], phi=xyphi[-1][other][2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 1\n",
    "\n",
    "if len(u_guess) <= ITER:\n",
    "    u_guess.append({})\n",
    "else:\n",
    "    u_guess[ITER] = {}\n",
    "\n",
    "for other in OTHERS:\n",
    "    u_guess[ITER][other] = u_grid(ROOT, other, spills, *u_guess[ITER-1][other], 4e-3, 11, nmax=5, alpha=0.05, \\\n",
    "           dx=xyphi[-1][other][0], dy=xyphi[-1][other][1], phi=xyphi[-1][other][2], visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 2\n",
    "\n",
    "if len(u_guess) <= ITER:\n",
    "    u_guess.append({})\n",
    "else:\n",
    "    u_guess[ITER] = {}\n",
    "\n",
    "for other in OTHERS:\n",
    "    u_guess[ITER][other] = u_grid(ROOT, other, spills, *u_guess[ITER-1][other], 1e-3, 11, nmax=7, alpha=0.05, \\\n",
    "           dx=xyphi[-1][other][0], dy=xyphi[-1][other][1], phi=xyphi[-1][other][2], visualize='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 3\n",
    "\n",
    "if len(u_guess) <= ITER:\n",
    "    u_guess.append({})\n",
    "else:\n",
    "    u_guess[ITER] = {}\n",
    "\n",
    "def f_u(x, p1, p2, spills, dx, dy, phi):\n",
    "    return 1 - score_spills(p1, p2, *spills, dx=dx, dy=dy, phi=phi, \\\n",
    "                            ux=x[0], uxy=x[1], uy=x[2], \n",
    "                            nmax=10, alpha=0.03)\n",
    "\n",
    "for other in OTHERS:\n",
    "    guess = np.array(u_guess[ITER-1][other])\n",
    "    progress = [guess]\n",
    "    res = scipy.optimize.minimize(f_u, guess, args=(ROOT, other, spills, *xyphi[-1][other]), method='Nelder-Mead', \\\n",
    "                                  options={'xatol': 1e-5}, callback=lambda xk: progress.append(xk))\n",
    "    \n",
    "    progress_arr = np.array(progress).transpose()\n",
    "    \n",
    "    xmin = np.amin(progress_arr[0])\n",
    "    xmax = np.amax(progress_arr[0])\n",
    "    ymin = np.amin(progress_arr[2])\n",
    "    ymax = np.amax(progress_arr[2])\n",
    "    \n",
    "    plt.xlim(xmin-(xmax-xmin)/6, xmax+(xmax-xmin)/6)\n",
    "    plt.ylim(ymin-(ymax-ymin)/6, ymax+(ymax-ymin)/6)\n",
    "    \n",
    "    plt.title(r'$(u_x, u_y, u_{xy})$ Convergence: ' + '({}, {})'.format(ROOT[:6], other[:6]))\n",
    "    plt.xlabel(r'$u_x$')\n",
    "    plt.ylabel(r'$u_y$')\n",
    "    plt.scatter(progress_arr[0], progress_arr[2], c=progress_arr[1], \\\n",
    "                s=160*np.arange(progress_arr.shape[1]+1)[::-1] / progress_arr.shape[1], cmap='winter', marker='x')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    if res.success:\n",
    "        print(\"Success!\")\n",
    "        u_guess[ITER][other] = res.x\n",
    "    else: print(\"Failed:\", res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do an optimization with all six parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_tot(x, p1, p2, spills):\n",
    "    return 1 - score_spills(p1, p2, *spills, dx=x[0], dy=x[1], phi=x[2], \\\n",
    "                            ux=x[3], uxy=x[4], uy=x[5], \n",
    "                            nmax=15, alpha=0.05)\n",
    "\n",
    "alignments = {ROOT: (0,0,0,0,0,0)}\n",
    "\n",
    "for other in OTHERS:\n",
    "    guess = np.array([*xyphi[-1][other], *u_guess[-1][other]])\n",
    "    res = scipy.optimize.minimize(f_tot, guess, args=(ROOT, other, spills), method='Nelder-Mead', \\\n",
    "                                  options={'xatol': 1e-5}, callback=lambda xk: print(xk))\n",
    "    \n",
    "    if res.success:\n",
    "        print(\"Success!\")\n",
    "        alignments[other] = res.x\n",
    "    else: print(\"Failed:\", res.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "for p in PHONE_LIST:\n",
    "    print('{}:'.format(p))\n",
    "    print(\"Measured: \", alignments[p])\n",
    "    print(\"Truth:\", align_truth[p])\n",
    "    print(\"Diff:\", alignments[p] - align_truth[p])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output corrected coordinates\n",
    "\n",
    "# TODO: GET RID OF THIS\n",
    "alignments = align_truth\n",
    "\n",
    "os.makedirs(os.path.join(DIR, 'align'), exist_ok=True)\n",
    "for iphone in PHONE_LIST:\n",
    "    \n",
    "    for spl in spills:\n",
    "        n_spl = 0\n",
    "        for t in spl[iphone]:\n",
    "\n",
    "            fspl = Spill.get_file(iphone, t, 'cluster')\n",
    "\n",
    "            x_sensor = (fspl['x'] + 0.5 - RES_X / 2) * PIX_SIZE\n",
    "            y_sensor = (fspl['y'] + 0.5 - RES_Y / 2) * PIX_SIZE\n",
    "            \n",
    "            x_lab, y_lab = sensor_map(x_sensor, y_sensor, *alignments[iphone])\n",
    "            \n",
    "            intersect_dict = {}\n",
    "            for jphone in PHONE_LIST:\n",
    "                if iphone == jphone: continue\n",
    "                    \n",
    "                xj, yj = inverse_map(x_lab, y_lab, *alignments[jphone])   \n",
    "                \n",
    "                intersect_dict[jphone] = (np.abs(xj) < LIM_X) & (np.abs(yj) < LIM_Y)\n",
    "                \n",
    "            np.savez('{}/align/{}_p{}_t{}.npz'.format(DIR, PREFIX, iphone, t), \n",
    "                     x=x_lab, \n",
    "                     y=y_lab,\n",
    "                     t=fspl['t'], \n",
    "                     particles=fspl['particles'],\n",
    "                     align=alignments[iphone], \n",
    "                     **intersect_dict)\n",
    "            fspl.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Efficiency #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general strategy for finding efficiency is simple enough.  We want to count the total number of coincidences on each sensor pair, subtract out the noise of different particles hitting the same region, and divide by the average number of particles that hit each sensor during the overlapping part of the frame.  Assuming the alignment has been done well, counting the coincidences should be easy.  For the background, if we have $N$ and $M$ uncorrelated hits on a pair of sensors, each with a distribution $P(\\vec{r})$:\n",
    "\n",
    "\\begin{equation}\n",
    "E(n_\\mathrm{coinc})=NM \\int d^2\\vec{r_1} P(\\vec{r_1}) \\int d^2\\vec{r_2} P(\\vec{r_2}) \\ \\ \\Theta\\left(r_0 - \\left|\\vec{r_1} - \\vec{r_2}\\right|\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Theta$ is the Heaviside function and $r_0$ is a threshold distance which determines whether two points are considered a coincidence.  If $P(\\vec{r})$ is roughly constant on the scale of $r_0$, we can treat it like a Delta function:\n",
    "\n",
    "\\begin{equation}\n",
    "E(n_\\mathrm{coinc}) \\approx NM \\int d^2\\vec{r} P(\\vec{r})^2\n",
    "\\end{equation}\n",
    "\n",
    "and this integral can be evaluated numerically on the dataset.  In general, if we are finding coincidences among $n$ sensors with occupancies $N_1, \\ldots N_n$, we can extend this as:\n",
    "\n",
    "\\begin{equation}\n",
    "E(n_\\mathrm{coinc}) \\approx \\prod_{i=1}^n N_i \\int d^2\\vec{r_1} P(\\vec{r_1})^n\n",
    "\\end{equation}\n",
    "\n",
    "However, for higher-order correlations, we need to take into account not just random noise, but also hits on two sensors coinciding with noise on others, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to find the average number of particles hitting a single frame in the overlapping time window, we note that if the beam is on for the full duration of all frames (with occupancies $\\{N_i\\}$), which overlap for some fraction $f$ of the total frame duration:\n",
    "\n",
    "\\begin{equation}\n",
    "n_\\mathrm{overlap} \\approx f \\bar{N}\n",
    "\\end{equation}\n",
    "\n",
    "On the other hand, if the beam starts or stops during one or more frames, but other frames receive the full intensity (i.e. the start or stop does not occur in the overlap window):\n",
    "\n",
    "\\begin{equation}\n",
    "n_\\mathrm{overlap} \\approx f \\max\\{M_i\\},\n",
    "\\qquad\n",
    "\\frac{N_{min}}{N_{max}} > f\n",
    "\\end{equation}\n",
    "\n",
    "and if the start or stop *does* occur in the overlap window:\n",
    "\n",
    "\\begin{equation}\n",
    "n_\\mathrm{overlap} \\approx \\min\\{N_i\\},\n",
    "\\qquad\n",
    "\\frac{N_{min}}{N_{max}} < f\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get alignment params\n",
    "aligns = {}\n",
    "for p in PHONE_LIST:\n",
    "    f0 = Spill.get_file(p, spills[0][p][0], filetype='align')\n",
    "    aligns[p] = f0['align']\n",
    "    f0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator through overlapping frames\n",
    "def gen_overlaps(spill, phones):\n",
    "\n",
    "    phones = np.array(phones)\n",
    "    times = [spill[p].copy() for p in phones]\n",
    "    t_ijk = np.array([t.pop(0) for t in times]).astype(int)\n",
    "    while True:\n",
    "        overlap = calculate_overlap(FPS, *t_ijk)        \n",
    "        if overlap:\n",
    "            yield t_ijk, overlap\n",
    "\n",
    "        # replace the earliest time, if possible\n",
    "        p_earliest = np.argmin(t_ijk)\n",
    "        if not times[p_earliest]: break\n",
    "        t_ijk[p_earliest] = times[p_earliest].pop(0)\n",
    "        \n",
    "def gen_overlaps_single(spill, phones):\n",
    "    phones = list(phones)\n",
    "    times = {p:spill[p].copy() for p in phones}\n",
    "    t_ijk = [times[p].pop(0) for p in phones]\n",
    "    \n",
    "    sort = np.argsort(t_ijk)\n",
    "    phones = [phones[arg] for arg in sort]\n",
    "    t_ijk = [t_ijk[arg] for arg in sort]\n",
    "    \n",
    "    initial_overlap = calculate_overlap(FPS, *t_ijk)\n",
    "    for t, phone in zip(t_ijk[:-1], phones[:-1]):\n",
    "        yield t, phone, 0\n",
    "    yield t_ijk[-1], phones[-1], initial_overlap\n",
    "    \n",
    "    while True:\n",
    "        # replace the earliest time, if possible\n",
    "        t_ijk = t_ijk[1:]\n",
    "        p_earliest = phones.pop(0)\n",
    "        \n",
    "        if not times[p_earliest]: break\n",
    "        t_new = times[p_earliest].pop(0)\n",
    "        \n",
    "        t_ijk.append(t_new)\n",
    "        phones.append(p_earliest)\n",
    "        overlap = calculate_overlap(FPS, *t_ijk)\n",
    "        \n",
    "        yield t_new, p_earliest, overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find geometrical factors\n",
    "X_MIN = {}\n",
    "Y_MIN = {}\n",
    "\n",
    "X_SIZE = {}\n",
    "Y_SIZE = {}\n",
    "\n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in clist:\n",
    "    \n",
    "        # get first frames\n",
    "        spl = spills[0]\n",
    "        f_ij = [Spill.get_file(p, spl[p][0], filetype='align') for p in c]\n",
    "\n",
    "        x_offsets = [f['align'][0] for f in f_ij]\n",
    "        y_offsets = [f['align'][1] for f in f_ij]\n",
    "        max_phi, max_ux, max_uxy, max_uy = np.amax(np.abs([f['align'][2:] for f in f_ij]), axis=0)\n",
    "\n",
    "        dx = np.diff(x_offsets)[0] / PIX_SIZE\n",
    "        dy = np.diff(y_offsets)[0] / PIX_SIZE\n",
    "\n",
    "        X_MIN[c] = max(x_offsets)-(RES_X*(max_ux+np.cos(max_phi)) + RES_Y*(max_uxy+np.sin(max_phi)))/2*PIX_SIZE\n",
    "        Y_MIN[c] = max(y_offsets)-(RES_Y*(max_uy+np.cos(max_phi)) + RES_X*(max_uxy+np.sin(max_phi)))/2*PIX_SIZE\n",
    "\n",
    "        X_SIZE[c] = np.ceil(RES_X*(max_ux+np.cos(max_phi))+RES_Y*(max_uxy+np.sin(max_phi)) - np.abs(dx)).astype(int)\n",
    "        Y_SIZE[c] = np.ceil(RES_Y*(max_uy+np.cos(max_phi))+RES_X*(max_uxy+np.sin(max_phi)) - np.abs(dy)).astype(int)\n",
    "\n",
    "        for f in f_ij: f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# locate the interior of the intersection in the root frame\n",
    "INTERIOR = {}\n",
    "\n",
    "bx = np.arange(0.5, RES_X, 1/np.sqrt(2))\n",
    "by = np.arange(0.5, RES_Y, 1/np.sqrt(2))\n",
    "\n",
    "bxi, byi = np.meshgrid(bx, by)\n",
    "\n",
    "bxi = PIX_SIZE * (bxi - RES_X / 2).flatten()\n",
    "byi = PIX_SIZE * (byi - RES_Y / 2).flatten()\n",
    "\n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in clist:\n",
    "        interior_coords = []\n",
    "        for p in c:\n",
    "            interior_x, interior_y = sensor_map(bxi, byi, *aligns[p])\n",
    "            for pj in (c - set([p])):\n",
    "                bxj, byj = inverse_map(interior_x, interior_y, *aligns[pj])\n",
    "                \n",
    "                intersect = (np.abs(bxj) < LIM_X) & (np.abs(byj) < LIM_Y)\n",
    "                interior_x = interior_x[intersect]\n",
    "                interior_y = interior_y[intersect]\n",
    "            \n",
    "            interior_x = ((interior_x - X_MIN[c]) / PIX_SIZE).astype(int)\n",
    "            interior_y = ((interior_y - Y_MIN[c]) / PIX_SIZE).astype(int)\n",
    "            interior_coords.append(interior_x + X_SIZE[c] * interior_y)\n",
    "            \n",
    "        interior_coords = np.unique(np.hstack(interior_coords))\n",
    "        interior_mat = np.zeros((X_SIZE[c], Y_SIZE[c]), dtype=bool)\n",
    "        interior_mat[interior_coords % X_SIZE[c], interior_coords // X_SIZE[c]] = True\n",
    "        \n",
    "        INTERIOR[c] = interior_mat\n",
    "        \n",
    "        plt.title(tuple([pk[:6] for pk in c]))\n",
    "        plt.imshow(interior_mat, extent=[0, X_SIZE[c], 0, Y_SIZE[c]], cmap='gray', vmin=0)\n",
    "        plt.show()\n",
    "        \n",
    "del bxi, byi, bxj, byj, interior_x, interior_y, interior_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find $\\int d^2\\vec{r_1} P(\\vec{r_1})^2$, we note that for a histogram (i.e. with Poisson statistics), $E(N(N-1)) = E(N^2) - E(N) = E((N-\\bar{N})^2)) + E(N)^2 - E(N) = \\sigma^2 + \\mu^2 - \\mu = \\mu^2$\n",
    "\n",
    "Similar methods can be employed with higher moments to achieve the result:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\mu^n = E\\left(N(N-1)\\ldots(N-n+1)\\right)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now test precision of effective area measurements\n",
    "c0 = COMBINATIONS[-1][0]\n",
    "bin_sz_options = (1,2,3,4,5,6,8,12,15)\n",
    "\n",
    "profiles = {p: 0 for p in c0}\n",
    "profile_temp = 0\n",
    "for p in c0:\n",
    "    for i,spl in enumerate(spills):\n",
    "        for t in spl[p]:\n",
    "            f = Spill.get_file(p, t, filetype='align')\n",
    "            if not f['x'].size: continue\n",
    "                \n",
    "            intersect = np.ones(f['x'].size, dtype=bool)\n",
    "            for iphone in c0:\n",
    "                if iphone == p: continue\n",
    "                intersect &= f[iphone]\n",
    "            \n",
    "            x = (f['x'][intersect] - X_MIN[c0]) / PIX_SIZE\n",
    "            y = (f['y'][intersect] - Y_MIN[c0]) / PIX_SIZE\n",
    "\n",
    "            # dither\n",
    "            x += np.random.random(x.size) - 0.5\n",
    "            y += np.random.random(x.size) - 0.5\n",
    "\n",
    "            profile_temp += csr_matrix((np.ones(x.size), (x, y)), shape=(X_SIZE[c0], Y_SIZE[c0]))\n",
    "            \n",
    "            f.close()\n",
    "            \n",
    "        if not (i+1)%10 or i==len(spills)-1:\n",
    "            profiles[p] += profile_temp.toarray()\n",
    "            profile_temp = 0\n",
    "\n",
    "\n",
    "plt_x = []\n",
    "plt_y = []\n",
    "\n",
    "for bin_sz in bin_sz_options:\n",
    "    mu_2 = []\n",
    "    for p in c0:\n",
    "        xpad = (bin_sz - X_SIZE[c0] % bin_sz) % bin_sz\n",
    "        ypad = (bin_sz - Y_SIZE[c0] % bin_sz) % bin_sz\n",
    "        \n",
    "        profile_pad = np.pad(profiles[p], ((0, xpad), (0, ypad)), 'constant')\n",
    "        profile_ds = profile_pad.reshape(profile_pad.shape[0]//bin_sz, bin_sz, profile_pad.shape[1]//bin_sz, bin_sz).sum((1,3))\n",
    "\n",
    "        mu_2.append((profile_ds * (profile_ds-1)).sum() / (profile_ds.sum()**2 * bin_sz**2))\n",
    "    \n",
    "    plt_x.append(bin_sz)\n",
    "    plt_y.append(mu_2)\n",
    "        \n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "for ip, p in enumerate(c0):\n",
    "    plt.plot(plt_x, [y[ip] for y in plt_y], label=p[:6], linestyle=':')\n",
    "plt.plot(plt_x, np.mean(np.array(plt_y), axis=1), label='Mean')\n",
    "#plt.semilogx()\n",
    "\n",
    "plt.title(r'Calculating $\\int P(\\vec{r})^2 d\\vec{r}$')\n",
    "plt.xlabel(r'$n_{bins}$')\n",
    "plt.ylabel(r'$1/A_{eff}$')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del profile_temp, profile_pad, profile_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the effective area\n",
    "\n",
    "BEAM_PROFILE_CORR = {}\n",
    "BEAM_PROFILE_VAR = {}\n",
    "\n",
    "downsample = 12\n",
    "\n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in clist:\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        bx = np.arange(X_SIZE[c])\n",
    "        by = np.arange(Y_SIZE[c])\n",
    "        bxx, byy = np.meshgrid(bx, by)\n",
    "        \n",
    "        for p in c:\n",
    "            \n",
    "            profile = 0\n",
    "            profile_temp = 0\n",
    "            \n",
    "            for i,spl in enumerate(spills):\n",
    "                for t in spl[p]:\n",
    "                    f = Spill.get_file(p, t, filetype='align')\n",
    "                    if not f['x'].size: continue\n",
    "            \n",
    "                    intersect = np.ones(f['x'].size, dtype=bool)\n",
    "                    for iphone in c:\n",
    "                        if iphone == p: continue\n",
    "                        intersect &= f[iphone]\n",
    "                        \n",
    "                    x = (f['x'][intersect] - X_MIN[c]) / PIX_SIZE\n",
    "                    y = (f['y'][intersect] - Y_MIN[c]) / PIX_SIZE\n",
    "                        \n",
    "                    # dither\n",
    "                    x += np.random.random(x.size) - 0.5\n",
    "                    y += np.random.random(y.size) - 0.5\n",
    "\n",
    "                    profile_temp += csr_matrix((np.ones(x.size), (x, y)), shape=(X_SIZE[c], Y_SIZE[c]))\n",
    "\n",
    "                    f.close()\n",
    "\n",
    "                if not (i+1)%10 or i==len(spills)-1:\n",
    "                    profile += profile_temp.toarray()\n",
    "                    profile_temp = 0\n",
    "        \n",
    "            profile_sum = profile.sum()\n",
    "            mu_n = 1\n",
    "            p_results = []\n",
    "        \n",
    "            for i in range(len(c)):\n",
    "                mu_n *= profile - i\n",
    "                if i:\n",
    "                    p_results.append(mu_n.sum() / profile_sum**(i+1))\n",
    "                    \n",
    "            results.append(p_results)\n",
    "                \n",
    "\n",
    "        BEAM_PROFILE_CORR[c] = np.mean(np.array(results), axis=0)\n",
    "        BEAM_PROFILE_VAR[c] = np.var(results, axis=0) / len(results)\n",
    "        \n",
    "        # make a plottable version\n",
    "        xpad = (downsample - X_SIZE[c] % downsample) % downsample\n",
    "        ypad = (downsample - Y_SIZE[c] % downsample) % downsample\n",
    "        \n",
    "        profile_pad = np.pad(profile, ((0, xpad), (0, ypad)), 'constant')\n",
    "        profile_ds = profile_pad.reshape(profile_pad.shape[0]//downsample, downsample, \\\n",
    "                                             profile_pad.shape[1]//downsample, downsample).sum((1,3))\n",
    "        \n",
    "        plt.title(list(map(lambda p: p[:6], c)))\n",
    "        plt.imshow(profile_ds, cmap='plasma', extent=[0, profile_pad.shape[0], 0, profile_pad.shape[1]], vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "        print('A_tot = {}'.format(np.count_nonzero(profile_ds) * downsample**2))\n",
    "        print('A_eff = {}'.format(np.array(BEAM_PROFILE_CORR[c])**(-1/(np.arange(1, len(c))))))\n",
    "        print('Fractional err:', np.sqrt(BEAM_PROFILE_VAR[c]) / BEAM_PROFILE_CORR[c])\n",
    "\n",
    "del profile, profile_temp, profile_pad, profile_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# at this point, we can measure the distribution of distances between hits\n",
    "XY_HISTS = {}\n",
    "\n",
    "survival_size = 100\n",
    "xy_size = 15\n",
    "\n",
    "# we only sample the interior to avoid edge effects\n",
    "def find_edge(binary_image, n):\n",
    "    binary_padded = np.pad(binary_image, ((n, n),(n,n)), 'constant')\n",
    "    interior = binary_padded.copy()\n",
    "    for roll in range(-n, n+1):\n",
    "        for ax in (0,1):\n",
    "            interior &= np.roll(binary_padded, roll, axis=ax) \n",
    "        \n",
    "    return binary_image ^ interior[n:-n, n:-n]\n",
    "\n",
    "xy_r = xy_size // 2\n",
    "for c in COMBINATIONS[2]:\n",
    "    survival_tot = np.zeros(survival_size)\n",
    "    XY_HISTS[c] = np.zeros((xy_size, xy_size))\n",
    "    \n",
    "    hist_interior = INTERIOR[c] & np.logical_not(find_edge(INTERIOR[c], 2*xy_r))\n",
    "    \n",
    "    for spl in spills[:30]:\n",
    "        tuple_c = tuple(sorted(c))\n",
    "        for times, overlap in gen_overlaps(spl, tuple_c):\n",
    "            f_ij = [Spill.get_file(p, t, filetype='align') for p, t in zip(tuple_c, times)]\n",
    "\n",
    "            intersect1 = np.ones(f_ij[0]['x'].size, dtype=bool)\n",
    "            intersect2 = np.ones(f_ij[1]['x'].size, dtype=bool)\n",
    "            for iphone in c:\n",
    "                if iphone != tuple_c[0]:\n",
    "                    intersect1 &= f_ij[0][iphone]\n",
    "                if iphone != tuple_c[1]:\n",
    "                    intersect2 &= f_ij[1][iphone]\n",
    "            \n",
    "            x1 = (f_ij[0]['x'][intersect1] - X_MIN[c]) / PIX_SIZE\n",
    "            y1 = (f_ij[0]['y'][intersect1] - Y_MIN[c]) / PIX_SIZE\n",
    "            x2 = (f_ij[1]['x'][intersect2] - X_MIN[c]) / PIX_SIZE\n",
    "            y2 = (f_ij[1]['y'][intersect2] - Y_MIN[c]) / PIX_SIZE\n",
    "            \n",
    "            for f in f_ij: f.close()\n",
    "            \n",
    "            # now cut out the edges on x1\n",
    "            edge_cut = hist_interior[x1.astype(int), y1.astype(int)]\n",
    "            \n",
    "            x1 = x1[edge_cut]\n",
    "            y1 = y1[edge_cut]\n",
    "            \n",
    "            xmin = 0\n",
    "            xmax = X_SIZE[c]\n",
    "            ymin = 0\n",
    "            ymax = Y_SIZE[c]\n",
    "\n",
    "            divx = (xmax - xmin) / NDIVS\n",
    "            divy = (ymax - ymin) / NDIVS\n",
    "\n",
    "            # make interlaced cells\n",
    "            x1_cells, y1_cells = divide_points(x1, y1, NDIVS, limits=(xmin, xmax, ymin, ymax))\n",
    "            x2_cells, y2_cells = divide_points(x2, y2, NDIVS+1, limits=(xmin - divx/2, xmax + divx/2, ymin-divy/2, ymax+divy/2))\n",
    "\n",
    "            for i,j in np.ndindex(NDIVS, NDIVS):\n",
    "                group1 = (x1_cells == i) & (y1_cells == j) \n",
    "                if not group1.sum():\n",
    "                    continue\n",
    "\n",
    "                group2 = ((x2_cells == i) | (x2_cells == i+1)) & ((y2_cells == j) | (y2_cells == j+1))\n",
    "                if not group2.sum():\n",
    "                    continue\n",
    "\n",
    "                rsquared_min = np.amin((x1[group1] - x2[group2].reshape(-1,1))**2 \\\n",
    "                                       + (y1[group1] - y2[group2].reshape(-1,1))**2, axis=0)\n",
    "\n",
    "                density = group2.sum() / (4 * divx * divy)\n",
    "                survival = np.exp(-rsquared_min * np.pi * density)\n",
    "\n",
    "                survival_tot += np.histogram(survival, bins=survival_size)[0]\n",
    "\n",
    "                xy_vals = np.dstack([x1[group1] - x2[group2].reshape(-1,1), \\\n",
    "                                     y1[group1] - y2[group2].reshape(-1,1)]).reshape(-1, 2).transpose()\n",
    "\n",
    "                XY_HISTS[c] += np.histogram2d(xy_vals[0], xy_vals[1], \\\n",
    "                                bins=(np.arange(-xy_r-0.5, xy_r + 1.5), np.arange(-xy_r - 0.5, xy_r + 1.5)))[0]\n",
    "\n",
    "            XY_HISTS[c] -= x1.size * x2.size * BEAM_PROFILE_CORR[c]\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Min distances: ({}, {})'.format(tuple_c[0][:6], tuple_c[1][:6]))\n",
    "    plt.xlabel('1-cdf')\n",
    "    plt.hist(np.linspace(0, 1, survival_tot.size), weights=survival_tot, bins=survival_tot.size)\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(-xy_r, xy_r + 1), np.arange(-xy_r, xy_r+1))\n",
    "    plt.subplot(122)\n",
    "    plt.title('Hit separation: ({}, {})'.format(tuple_c[0][:6], tuple_c[1][:6]))\n",
    "    plt.xlabel(r'$\\Delta x$ (pixels)')\n",
    "    plt.ylabel(r'$\\Delta y$ (pixels)')\n",
    "    \n",
    "    plt.imshow(XY_HISTS[c], extent=[-xy_r-0.5, xy_r+0.5, -xy_r-0.5, xy_r+0.5], norm=LogNorm())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "del hist_interior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_SIZE_LIST = [(9,9),(7,7),(5,5),(5,5),(7,7),(5,5)]\n",
    "CONV_SIZES = {COMBINATIONS[2][i]: CONV_SIZE_LIST[i] for i in range(len(COMBINATIONS[2]))} \n",
    "\n",
    "# now find the edge region to exclude\n",
    "EDGE = {}\n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in clist:\n",
    "        EDGE[c] = find_edge(INTERIOR[c], max([max(conv) for conv in CONV_SIZES.values()]) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility methods\n",
    "\n",
    "def convolve_sparse(x, y, csize, shape):\n",
    "    \n",
    "    center = np.array(csize) // 2\n",
    "    conv_x = []\n",
    "    conv_y = []\n",
    "    \n",
    "    for idx, idy in np.ndindex(csize):\n",
    "        ix = x + idx-center[0]\n",
    "        iy = y + idy-center[1]\n",
    "        valid = (ix > 0) & (iy > 0) & (ix < shape[0]) & (iy < shape[1])\n",
    "\n",
    "        conv_x.append(ix[valid])\n",
    "        conv_y.append(iy[valid])\n",
    "    \n",
    "    conv_x = np.hstack(conv_x)\n",
    "    conv_y = np.hstack(conv_y)\n",
    "    \n",
    "    return csr_matrix((np.ones(conv_x.size), (conv_x, conv_y)), shape=shape)\n",
    "    \n",
    "    \n",
    "def partition(iterable):\n",
    "    collection = list(iterable)\n",
    "    if len(collection) == 1:\n",
    "        yield [ collection ]\n",
    "        return\n",
    "\n",
    "    first = collection[0]\n",
    "    for smaller in partition(collection[1:]):\n",
    "        # insert `first` in each of the subpartition's subsets\n",
    "        for n, subset in enumerate(smaller):\n",
    "            yield smaller[:n] + [[ first ] + subset]  + smaller[n+1:]\n",
    "        # put `first` in its own subset \n",
    "        yield [ [ first ] ] + smaller\n",
    "        \n",
    "def superpartition(collection, n_sets):\n",
    "    for idx in map(np.array, np.ndindex(*np.repeat(2**n_sets - 1, len(collection)))):\n",
    "        include = (idx + 1) // 2**np.arange(n_sets).reshape(-1, 1) % 2 == 1\n",
    "        if not np.all(include.sum(axis=1)): continue\n",
    "        part = [list(np.array(collection)[include[i]]) for i in range(n_sets)]\n",
    "        # make sure we only get each grouping once\n",
    "        if part != sorted(part): continue\n",
    "        yield part\n",
    "        \n",
    "def powerset(iterable, min_size=0, max_size=None):\n",
    "    s = list(iterable)\n",
    "    if not max_size: max_size = len(s)\n",
    "    return map(frozenset, it.chain.from_iterable(it.combinations(s, r) for r in range(min_size, max_size+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each combination of phones, determine the fastest way to convolve\n",
    "OPTIMAL_P = {}\n",
    "for subset in powerset(PHONE_LIST, min_size=2):\n",
    "    pi_best = None\n",
    "    for pi in subset:\n",
    "        total_conv = 0\n",
    "        for pj in subset:\n",
    "            if pi is pj: continue\n",
    "            total_conv += np.product(CONV_SIZES[frozenset([pi, pj])])\n",
    "            \n",
    "        if pi_best == None or total_conv < min_conv:\n",
    "            min_conv = total_conv\n",
    "            pi_best = pi\n",
    "            \n",
    "    OPTIMAL_P[subset] = pi_best \n",
    "    \n",
    "#HIT_FRAC = {}\n",
    "#for c in COMBINATIONS[2]:\n",
    "#    xy_hist = XY_HISTS[c][0] - BEAM_PROFILE_CORR[c][0]**XY_HISTS[c][1]\n",
    "#    conv_size = np.array(CONV_SIZES[c])\n",
    "    \n",
    "#    xstart, ystart = np.array(xy_hist.shape) // 2 - conv_size // 2\n",
    "#    xend, yend = np.array(xy_hist.shape) // 2 + conv_size // 2 + 1\n",
    "    \n",
    "#    HIT_FRAC[c] = xy_hist[xstart:xend, ystart:yend].sum() / xy_hist.sum()\n",
    "#for k,v in OPTIMAL_P.items():\n",
    "#    print([ki[:6] for ki in k], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get effective convolution sizes as well\n",
    "CONV_FACTORS = {}\n",
    "\n",
    "min_convolutions = {}\n",
    "\n",
    "for pi in PHONE_LIST:\n",
    "    p_other = PHONE_LIST.copy()\n",
    "    p_other.remove(pi)\n",
    "    for s in map(list, powerset(p_other, min_size=1)):\n",
    "        s_conv = np.array([CONV_SIZES[frozenset([pi, si])] for si in s])\n",
    "        min_conv_idx = np.argmin(np.product(s_conv, axis=1))\n",
    "        min_conv = s_conv[min_conv_idx]\n",
    "                                \n",
    "        conv_tot = 0\n",
    "        \n",
    "        #printarr = np.zeros(min_conv)\n",
    "        # iterate over positions for particle to be incident on min_conv sensor\n",
    "        for ixy in map(lambda a: np.array(a) - (min_conv // 2), np.ndindex(tuple(min_conv))):\n",
    "            conv_contribs = [1]\n",
    "            \n",
    "            # now multiply by probabilities for particle to be in convolution windows on other sensors\n",
    "            for s_idx, pj in enumerate(s):\n",
    "                if s_idx == min_conv_idx: continue\n",
    "                \n",
    "                xy_idx = frozenset([s[min_conv_idx], pj])\n",
    "                xy_hist = XY_HISTS[xy_idx]\n",
    "                xy_size = np.array(CONV_SIZES[xy_idx]) \n",
    "                \n",
    "                xstart, ystart = np.array(xy_hist.shape) // 2 + ixy - s_conv[s_idx] // 2\n",
    "                xend = xstart + s_conv[s_idx][0]\n",
    "                yend = ystart + s_conv[s_idx][1]\n",
    "                \n",
    "                hist_cut = (xy_hist[xstart:xend, ystart:yend]).sum() / xy_hist.sum()\n",
    "\n",
    "                conv_contribs.append(hist_cut)\n",
    "                        \n",
    "            # mean of upper (perfect correlation) and lower (no correlation) bounds\n",
    "            conv_tot += (np.product(conv_contribs) + min(conv_contribs)) / 2\n",
    "            #printarr[tuple(ixy + (min_conv // 2))] = min(conv_contribs)\n",
    "        \n",
    "        CONV_FACTORS[(pi, frozenset(s))] = conv_tot\n",
    "        min_convolutions[(pi, frozenset(s))] = np.product(min_conv)\n",
    "        \n",
    "        #print(printarr)\n",
    "        #print(conv_tot)\n",
    "        \n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in map(frozenset, clist):\n",
    "        pi = OPTIMAL_P[c]\n",
    "        pj_all = c - set([pi])\n",
    "        \n",
    "        print(pi[:6], [p[:6] for p in pj_all], min_convolutions[(pi, frozenset(pj_all))], CONV_FACTORS[(pi, frozenset(pj_all))])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NOISE_INTERSECT = {}\n",
    "\n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in clist:\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        bx = np.arange(X_SIZE[c])\n",
    "        by = np.arange(Y_SIZE[c])\n",
    "        bxx, byy = np.meshgrid(bx, by)\n",
    "        \n",
    "        for p in c:\n",
    "    \n",
    "            bxx_sensor, byy_sensor = inverse_map((bxx*PIX_SIZE + X_MIN[c]).flatten(), \\\n",
    "                                                 (byy*PIX_SIZE + Y_MIN[c]).flatten(), \\\n",
    "                                                 *aligns[p])\n",
    "\n",
    "            # now map to coordinates of the noise histograms\n",
    "            cut = (np.abs(bxx_sensor) < RES_X / 2 * PIX_SIZE) & (np.abs(byy_sensor) < RES_Y / 2 * PIX_SIZE)\n",
    "\n",
    "            bxx_sensor = (bxx_sensor / PIX_SIZE + RES_X / 2).astype(int)[cut]\n",
    "            byy_sensor = (byy_sensor / PIX_SIZE + RES_Y / 2).astype(int)[cut]\n",
    "\n",
    "            values_cut = NOISE[p][bxx_sensor, byy_sensor]\n",
    "\n",
    "            template = np.zeros((X_SIZE[c], Y_SIZE[c]))\n",
    "            template[bxx.flatten()[cut], byy.flatten()[cut]] = values_cut\n",
    "            template *= (INTERIOR[c] & np.logical_not(EDGE[c]))\n",
    "\n",
    "            NOISE_INTERSECT[(c, p)] = template\n",
    "\n",
    "            #plt.imshow(template, cmap='viridis', extent=[0, X_SIZE[c], 0, Y_SIZE[c]])\n",
    "            #plt.colorbar()\n",
    "            #plt.title('Noise: {}'.format(p[:6]))\n",
    "            #plt.show()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# this should work faster than a standard convolution for a sparse matrix\n",
    "eff_estimates = {}\n",
    "all_diffs = {}\n",
    "noise_diffs = {}\n",
    "single_diffs = {}\n",
    "    \n",
    "for clist in COMBINATIONS[2:]:\n",
    "    for c in map(frozenset, clist):\n",
    "        \n",
    "        n_all = []\n",
    "        n_single = []\n",
    "        \n",
    "        #q = []\n",
    "        \n",
    "        truth_all = []\n",
    "        truth_single = []\n",
    "        \n",
    "        for n_spills, spl in enumerate(spills):\n",
    "            print(\"{:.2f}%\".format(100*n_spills/len(spills)), end=\"\\r\")\n",
    "            \n",
    "            all_spl = 0\n",
    "            single_spl = 0\n",
    "            \n",
    "            #q_spl = 0\n",
    "            \n",
    "            truth_all_spl = 0\n",
    "            truth_single_spl = 0\n",
    "            \n",
    "            # keep a cache of results\n",
    "            n_coinc = {}\n",
    "            n_hits_interior = {}\n",
    "            n_hits_corr = {}\n",
    "            first_last = {p: True for p in c}\n",
    "            conv_ij = {p: {} for p in c}\n",
    "            \n",
    "            truth_particles = {}\n",
    "            \n",
    "            # one way to propagate the error in the beam profile estimate\n",
    "            #profile_corr = [np.random.normal(corr, np.sqrt(var)) for corr, var \\\n",
    "            #                in zip(BEAM_PROFILE_CORR[c], BEAM_PROFILE_VAR[c])]\n",
    "\n",
    "            for t, p, overlap in gen_overlaps_single(spl, c):\n",
    "                \n",
    "                # clear out old entries\n",
    "                conv_ij[p] = {}\n",
    "                \n",
    "                f = Spill.get_file(p, t, filetype='align')\n",
    "                \n",
    "                intersect = np.ones(f['x'].size, dtype=bool)\n",
    "                for iphone in c:\n",
    "                    if iphone == p: continue\n",
    "                    intersect &= f[iphone]\n",
    "                \n",
    "                x_new = ((f['x'][intersect] - X_MIN[c]) / PIX_SIZE).astype(int)\n",
    "                y_new = ((f['y'][intersect] - Y_MIN[c]) / PIX_SIZE).astype(int)\n",
    "                                \n",
    "                interior = np.logical_not(EDGE[c][x_new, y_new])\n",
    "                \n",
    "                n_coinc[frozenset([p])] = x_new.size\n",
    "                n_hits_interior[p] = interior.sum()\n",
    "                n_hits_corr[p] = n_hits_interior[p] - NOISE_INTERSECT[(c,p)].sum()\n",
    "                first_last[p] = (t in np.array(spl[p])[[0,-1]])\n",
    "\n",
    "                particles_intersect = f['particles'][intersect[:f['particles'].size]]\n",
    "                truth_particles[p] = particles_intersect[interior[:particles_intersect.size]] \\\n",
    "                                    if p == OPTIMAL_P[c] else particles_intersect\n",
    "                truth_single_spl += interior[:particles_intersect.size].sum() / len(c)\n",
    "                                \n",
    "                f.close()\n",
    "                \n",
    "                for subc in powerset(c, min_size=2):\n",
    "                    if not p in subc: continue\n",
    "                    \n",
    "                    # at the very least, we make the convolved matrix for p\n",
    "                    pi = OPTIMAL_P[subc]\n",
    "                    \n",
    "                    if not pi in conv_ij[p]:\n",
    "                        if p == pi:\n",
    "                            conv_ij[p][pi] = convolve_sparse(x_new[interior], y_new[interior], \\\n",
    "                                                           (1,1), shape=(X_SIZE[c], Y_SIZE[c]))\n",
    "                        else: \n",
    "                            conv_ij[p][pi] = convolve_sparse(x_new, y_new, CONV_SIZES[frozenset([pi, p])],\n",
    "                                                           shape=(X_SIZE[c], Y_SIZE[c]))\n",
    "                    \n",
    "                    \n",
    "                    pj_all = subc - set([pi])\n",
    "                    if not all([pi in conv_ij[pij] for pij in subc]): continue\n",
    "                        \n",
    "                    sparse_i = conv_ij[pi][pi].copy()\n",
    "                    \n",
    "                    for pj in pj_all:\n",
    "                        sparse_i = sparse_i.multiply(conv_ij[pj][pi])\n",
    "                \n",
    "                    # now we calculate the noise\n",
    "                    noise_tot = 0\n",
    "                    for part in partition(subc):\n",
    "                        \n",
    "                        order = len(part)\n",
    "                        if order == 1: continue # this is the signal term\n",
    "                            \n",
    "                        noise_contribution = BEAM_PROFILE_CORR[c][order-2] #(RES_X * RES_Y) ** float(1-order) #\n",
    "                        \n",
    "                        for s in map(frozenset, part):\n",
    "                            if s == frozenset([pi]):\n",
    "                                noise_contribution *= n_hits_interior[pi]\n",
    "                            else:\n",
    "                                noise_contribution *= n_coinc[s]\n",
    "                            \n",
    "                            if not pi in s:\n",
    "                                noise_contribution *= CONV_FACTORS[(pi, s)]\n",
    "                        \n",
    "                        noise_tot += noise_contribution\n",
    "                    \n",
    "                    n_coinc[subc] = (sparse_i.sum() - noise_tot) #/ hit_frac\n",
    "                \n",
    "                if not overlap: continue\n",
    "                \n",
    "                all_spl += n_coinc[c]\n",
    "                #q_spl += sparse_i.sum()\n",
    "                truth_all_spl += len(set.intersection(*map(set, truth_particles.values())))\n",
    "                \n",
    "                if np.any(list(first_last.values())):\n",
    "\n",
    "                    if min(n_hits_corr.values()) <= 0 \\\n",
    "                    or min(n_hits_corr.values()) / max(n_hits_corr.values()) > overlap:\n",
    "                        if np.all(list(first_last.values())):\n",
    "                            # either we're missing a frame, or this is a statistical fluke\n",
    "                            single_spl += overlap * max(n_hits_corr.values())\n",
    "                        else:\n",
    "                            single_spl += overlap * np.mean([n_hits_corr[p] for p in c if not first_last[p]])\n",
    "                    else:\n",
    "                        single_spl += min(n_hits_corr.values())\n",
    "\n",
    "                else:\n",
    "                    single_spl += overlap * np.mean(list(n_hits_corr.values()))\n",
    "                    \n",
    "            n_all.append(all_spl)\n",
    "            n_single.append(single_spl)\n",
    "            \n",
    "            #q.append(q_spl)\n",
    "            \n",
    "            truth_all.append(truth_all_spl)\n",
    "            truth_single.append(truth_single_spl)\n",
    "            \n",
    "        print(\"100% \", end=\"\\r\")\n",
    "        print(\"all:\", np.mean(n_all))\n",
    "        print(\"single\", np.mean(n_single))\n",
    "        print(\"truth all\", np.mean(truth_all))\n",
    "        print(\"truth single\", np.mean(truth_single))\n",
    "        #print(\"q\", np.mean(q))\n",
    "        #print(\"noise\", np.mean(q) - np.mean(n_all))\n",
    "        \n",
    "        # now find an unbiased ratio estimator (to first order) for each spill\n",
    "\n",
    "        eff_estimates[c] = np.array(n_all) / np.array(n_single)\n",
    "        all_diffs[c] = np.array(n_all) / np.array(truth_all) - 1\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.suptitle(tuple(map(lambda p: p[:6], c)))\n",
    "        plt.subplot(121)\n",
    "        plt.title('Efficiency by spill')\n",
    "        plt.hist(eff_estimates[c], bins=35)\n",
    "        plt.xlabel(r'$\\epsilon$')\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title('Numerator')\n",
    "        plt.hist(all_diffs[c], bins=35)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftruth = np.load('{}/{}_truth.npz'.format(DIR, PREFIX))\n",
    "\n",
    "weighted_efficiencies = []\n",
    "weighted_eff_vars = []\n",
    "\n",
    "for ic, clist in enumerate(COMBINATIONS[2:]):\n",
    "    means = []\n",
    "    variances = []\n",
    "    for c in clist:\n",
    "        mean_c = np.mean(eff_estimates[c])\n",
    "        var_c = np.var(eff_estimates[c]) / len(eff_estimates[c])\n",
    "        means.append(mean_c)\n",
    "        variances.append(var_c)\n",
    "        print(\"{}: {:.6f} +/- {:.6f}\".format(tuple(map(lambda p: p[:6], c)), mean_c, np.sqrt(var_c)))\n",
    "    \n",
    "    # now do a weighted average\n",
    "    means = np.array(means)\n",
    "    variances = np.array(variances)\n",
    "    \n",
    "    eff = np.sum(means / variances) / np.sum(1 / variances)\n",
    "    \n",
    "    # because of systematic error, we find the weighted sample variance here\n",
    "    if means.size == 1:\n",
    "        eff_var = variances[0]\n",
    "    else:\n",
    "        eff_var = np.sum((means-eff)**2 / variances) / np.sum(1 / variances)\n",
    "    \n",
    "    print(\"Total: {:.7f} +/- {:.7f}\".format(eff, np.sqrt(eff_var)))\n",
    "    print(\"Truth: {:.7f}\".format(np.dot(ftruth['eff']**(ic+2), PARTICLES) / np.dot(ftruth['eff'], PARTICLES)))\n",
    "    print()\n",
    "    weighted_efficiencies.append(eff)\n",
    "    weighted_eff_vars.append(eff_var)\n",
    "    \n",
    "ftruth.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $n$-point efficiency estimates generated here are just:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\epsilon^{(n)} = \\frac{\\sum_{i=1}^{k}\\epsilon_i^n \\phi_i}{\\sum_{i=1}^{k}\\epsilon_i \\phi_i}\n",
    "\\end{equation}$\n",
    "\n",
    "where $k$ is the number of particles, and $\\phi_i$ are the fluxes for each particle.  Dividing by the total flux $\\sum_{i=1}^k \\phi_i$ on both top and bottom:\n",
    "\n",
    "$\\begin{equation}\n",
    "\\epsilon^{(n)} = \\frac{\\sum_{i=1}^{k}\\epsilon_i^n f_i}{\\sum_{i=1}^{k}\\epsilon_i f_i}\n",
    "\\end{equation}$\n",
    "\n",
    "where $f_i$ denotes the known fractional flux of each particle.  This is a nonlinear system of equations which can be solved for each $\\epsilon_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(PARTICLES) == 1:\n",
    "    # just do a weighted average, taking the correct power of the efficiency\n",
    "    sum_wx = 0\n",
    "    sum_w = 0\n",
    "    for i in range(len(weighted_efficiencies)):\n",
    "        b = 1/(i+1)\n",
    "        eff = weighted_efficiencies[i]**b\n",
    "        eff_var = weighted_eff_vars[i] * b**2 / eff ** (2*i)\n",
    "        \n",
    "        sum_wx += eff / eff_var\n",
    "        sum_w += 1 / eff_var\n",
    "        \n",
    "    print(\"eff: {:.7} +/- {:.7}\".format(sum_wx/sum_w, np.sqrt(1/sum_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "COLZ = ['r','g','b']\n",
    "\n",
    "if len(PARTICLES) == 2:\n",
    "    A = np.linspace(0,1,100)\n",
    "    X, Y = np.meshgrid(A,A)\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "\n",
    "    plt.xlabel(r'$\\epsilon_1$')\n",
    "    plt.ylabel(r'$\\epsilon_2$')\n",
    "\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "    contours = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, eff in enumerate(weighted_efficiencies):\n",
    "        \n",
    "        def fn(x, y):\n",
    "            X = np.moveaxis(np.array([x, y]), 0, 2)\n",
    "            return np.dot(X**(i+2), PARTICLES) / np.dot(X, PARTICLES)\n",
    "        \n",
    "        contours.append(plt.contour(X, Y, fn(X, Y), [eff], colors=COLZ[i]))\n",
    "        labels.append('{} phones'.format(i+2))\n",
    "        \n",
    "    plt.legend(map(lambda cont: cont.legend_elements()[0][0], contours), labels)\n",
    "    plt.show()\n",
    "    \n",
    "elif len(PARTICLES) == 3:\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.gca(projection='3d')\n",
    "\n",
    "    ax.set_xlabel(r'$\\epsilon_1$')\n",
    "    ax.set_ylabel(r'$\\epsilon_2$')\n",
    "    ax.set_zlabel(r'$\\epsilon_3$')\n",
    "\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylim(1,0)\n",
    "    ax.set_zlim(0,1)\n",
    "    \n",
    "    u = np.linspace(0, np.pi / 2, 400)\n",
    "    v = np.linspace(0, np.pi / 2, 400)\n",
    "    \n",
    "    x0 = np.outer(np.sin(v), np.cos(u))\n",
    "    y0 = np.outer(np.sin(v), np.sin(u))\n",
    "    z0 = np.outer(np.cos(v), np.ones(u.size))\n",
    "    \n",
    "    X = np.moveaxis(np.array([x0, y0, z0]), 0, 2)\n",
    "    \n",
    "\n",
    "    r = []\n",
    "    \n",
    "    for i,eff in enumerate(weighted_efficiencies):\n",
    "        \n",
    "        r.append((eff * np.dot(X, PARTICLES) / np.dot(X**(i+2), PARTICLES))**(1/(i+1)))\n",
    "        \n",
    "    r = np.array(r)\n",
    "    args = np.argmax(r, axis=0) \n",
    "    \n",
    "    # slice on the max r values \n",
    "    idx = list(np.ogrid[[slice(r.shape[ax]) for ax in range(r.ndim) if ax != 0]])\n",
    "    idx.insert(0, args)\n",
    "    idx = tuple(idx)\n",
    "    \n",
    "    r = r[idx]\n",
    "    c = np.array([np.full_like(x0, c, dtype=str) for c in COLZ])[idx]\n",
    "    \n",
    "    e1 = r * x0\n",
    "    e2 = r * y0\n",
    "    e3 = r * z0\n",
    "    \n",
    "    ax.plot_surface(e1, e2, e3, rstride=1, cstride=1, facecolors=c)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    X, Y = np.meshgrid(np.linspace(0, np.pi/2, args.shape[0]), \n",
    "                       np.linspace(0, np.pi/2, args.shape[1]))\n",
    "    \n",
    "    # create a custom colormap\n",
    "    cm = LinearSegmentedColormap.from_list('rgb', [(1, 0, 0), (0, 1, 0), (0, 0, 1)], N=3)\n",
    "    plt.imshow(args[::-1,::-1], cmap=cm, \n",
    "               origin='lower', extent=[0, np.pi/2, 0, np.pi/2])\n",
    "    contour = plt.contour(X, Y, r[::-1, ::-1], cmap='gray')\n",
    "    plt.clabel(contour, fontsize=8)\n",
    "    plt.xlabel(r'$\\phi$')\n",
    "    plt.ylabel(r'$\\theta$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = (.3, .7)\n",
    "\n",
    "if len(PARTICLES) == 2:\n",
    "    plt.figure(figsize=(7,7))\n",
    "\n",
    "    plt.xlabel(r'$\\epsilon_1$')\n",
    "    plt.ylabel(r'$\\epsilon_2$')\n",
    "\n",
    "    plt.xlim(guess[0]-0.02, guess[0]+0.02)\n",
    "    plt.ylim(guess[1]-0.002, guess[1]+0.002)\n",
    "    \n",
    "    for i, eff, eff_var in zip(range(len(weighted_efficiencies)), weighted_efficiencies, weighted_eff_vars):\n",
    "        \n",
    "        def fn(x, y):\n",
    "            X = np.moveaxis(np.array([x, y]), 0, 2)\n",
    "            return np.dot(X**(i+2), PARTICLES) / np.dot(X, PARTICLES)\n",
    "        \n",
    "        if eff_var:\n",
    "            plt.contour(X, Y, fn(X, Y), [eff-np.sqrt(eff_var), eff, eff+np.sqrt(eff_var)], colors=COLZ[i])\n",
    "        else:\n",
    "            plt.contour(X, Y, fn(X, Y), [eff], colors=COLZ[i])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = (1.07, 1.21)\n",
    "\n",
    "def f(phitheta):\n",
    "    phi, theta = phitheta\n",
    "    \n",
    "    x0 = np.sin(theta) * np.cos(phi)\n",
    "    y0 = np.sin(theta) * np.sin(phi)\n",
    "    z0 = np.cos(theta)\n",
    "    \n",
    "    X = np.array([x0, y0, z0])\n",
    "\n",
    "    f2 = np.dot(X, PARTICLES) / np.dot(X**2, PARTICLES) * weighted_efficiencies[0]\n",
    "    f3 = np.dot(X**2, PARTICLES) / np.dot(X**3, PARTICLES) * weighted_efficiencies[1] / weighted_efficiencies[0]\n",
    "    f4 = np.dot(X**3, PARTICLES) / np.dot(X**4, PARTICLES) * weighted_efficiencies[2] / weighted_efficiencies[1]\n",
    "    \n",
    "    return np.array([f3 - f2, f4 - f2])\n",
    "\n",
    "guess_compl = np.pi/2 - np.array(guess)\n",
    "\n",
    "phi, theta = scipy.optimize.fsolve(f, guess_compl)\n",
    "print('\\u03C6 = {:.4f}'.format(np.pi/2 - phi))\n",
    "print('\\u03B8 = {:.4f}'.format(np.pi/2 - theta))\n",
    "print()\n",
    "\n",
    "x0 = np.sin(theta) * np.cos(phi)\n",
    "y0 = np.sin(theta) * np.sin(phi)\n",
    "z0 = np.cos(theta)\n",
    "\n",
    "X = np.array([x0, y0, z0])\n",
    "\n",
    "r = np.dot(X, PARTICLES) / np.dot(X**2, PARTICLES) * weighted_efficiencies[0]\n",
    "\n",
    "e1 = r * np.sin(theta) * np.cos(phi)\n",
    "e2 = r * np.sin(theta) * np.sin(phi)\n",
    "e3 = r * np.cos(theta)\n",
    "\n",
    "print('\\u03B51 = {:.5f}'.format(e1))\n",
    "print('\\u03B52 = {:.5f}'.format(e2))\n",
    "print('\\u03B53 = {:.5f}'.format(e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
